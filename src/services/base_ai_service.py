"""
Base AI Service - Service IA G√©n√©rique avec Rate Limiting Auto
Version am√©lior√©e avec gestion automatique des quotas et retry
"""

import asyncio
import concurrent.futures
import logging
from datetime import datetime

from pydantic import BaseModel, ValidationError

from src.core.ai import AnalyseurIA, ClientIA, RateLimitIA
from src.core.ai.cache import CacheIA
from src.core.errors import ErreurLimiteDebit, gerer_erreurs

logger = logging.getLogger(__name__)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# BASE AI SERVICE (AVEC RATE LIMITING AUTO)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


class BaseAIService:
    """
    Service IA de base avec fonctionnalit√©s communes

    Fonctionnalit√©s AUTO :
    - ‚úÖ Rate limiting avec retry intelligent
    - ‚úÖ Cache s√©mantique automatique
    - ‚úÖ Parsing JSON robuste
    - ‚úÖ Gestion d'erreurs unifi√©e
    - ‚úÖ Logging avec m√©triques
    """

    def __init__(
        self,
        client: ClientIA,
        cache_prefix: str = "ai",
        default_ttl: int = 3600,
        default_temperature: float = 0.7,
        service_name: str = "unknown",
    ):
        """
        Initialise le service IA

        Args:
            client: Client IA (ClientIA)
            cache_prefix: Pr√©fixe pour cl√©s cache
            default_ttl: TTL cache par d√©faut (secondes)
            default_temperature: Temp√©rature par d√©faut
            service_name: Nom du service (pour analytics)
        """
        self.client = client
        self.cache_prefix = cache_prefix
        self.default_ttl = default_ttl
        self.default_temperature = default_temperature
        self.service_name = service_name

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # APPELS IA AVEC RATE LIMITING AUTO + CACHE
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    @gerer_erreurs(afficher_dans_ui=True, valeur_fallback=None)
    async def call_with_cache(
        self,
        prompt: str,
        system_prompt: str = "",
        temperature: float | None = None,
        max_tokens: int = 1000,
        use_cache: bool = True,
        category: str | None = None,
    ) -> str | None:
        """
        Appel IA avec rate limiting + cache automatiques

        Args:
            prompt: Prompt utilisateur
            system_prompt: Instructions syst√®me
            temperature: Temp√©rature (None = default)
            max_tokens: Tokens max
            use_cache: Utiliser le cache
            category: Cat√©gorie pour cache

        Returns:
            R√©ponse IA ou None si erreur

        Raises:
            ErreurLimiteDebit: Si quota atteint
        """
        # ‚úÖ V√©rifier que le client IA est disponible
        if self.client is None:
            logger.warning(f"‚ö†Ô∏è Client IA indispo ({self.service_name})")
            return None

        temp = temperature if temperature is not None else self.default_temperature
        cache_category = category or self.cache_prefix

        # ‚úÖ V√©rifier cache AVANT rate limit (√©conomise les quotas)
        if use_cache:
            cached = CacheIA.obtenir(
                prompt=prompt,
                systeme=system_prompt,
                temperature=temp,
            )

            if cached:
                logger.info(f"‚úÖ Cache HIT ({cache_category}) - Quota √©conomis√© !")
                return cached

        # ‚úÖ V√©rifier rate limit AUTO
        autorise, msg = RateLimitIA.peut_appeler()
        if not autorise:
            logger.warning(f"‚è≥ Rate limit: {msg}")
            raise ErreurLimiteDebit(msg, message_utilisateur=msg)

        # Appel IA
        start_time = datetime.now()

        response = await self.client.appeler(
            prompt=prompt,
            prompt_systeme=system_prompt,
            temperature=temp,
            max_tokens=max_tokens,
            utiliser_cache=False,  # On g√®re le cache nous-m√™mes
        )

        duration = (datetime.now() - start_time).total_seconds()

        # ‚úÖ Enregistrer appel AUTO (avec m√©triques)
        RateLimitIA.enregistrer_appel(
            service=self.service_name,
            tokens_utilises=len(response) if response else 0,  # Approximation
        )

        logger.info(
            f"‚úÖ Appel IA r√©ussi ({self.service_name}) - "
            f"{duration:.2f}s - {len(response) if response else 0} chars"
        )

        # Sauvegarder dans cache
        if use_cache and response:
            CacheIA.definir(
                prompt=prompt,
                reponse=response,
                systeme=system_prompt,
                temperature=temp,
            )

        return response

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PARSING AVEC VALIDATION
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    @gerer_erreurs(afficher_dans_ui=True, valeur_fallback=None)
    async def call_with_parsing(
        self,
        prompt: str,
        response_model: type[BaseModel],
        system_prompt: str = "",
        temperature: float | None = None,
        max_tokens: int = 1000,
        use_cache: bool = True,
        fallback: dict | None = None,
    ) -> BaseModel | None:
        """
        Appel IA avec parsing automatique vers mod√®le Pydantic

        Rate limiting + cache AUTO int√©gr√©s !
        """
        # Appel IA (rate limiting d√©j√† g√©r√©)
        response = await self.call_with_cache(
            prompt=prompt,
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            use_cache=use_cache,
        )

        if not response:
            return None

        # Parser avec AnalyseurIA
        try:
            parsed = AnalyseurIA.analyser(
                reponse=response, modele=response_model, valeur_secours=fallback, strict=False
            )

            logger.info(f"‚úÖ Parsing r√©ussi: {response_model.__name__}")
            return parsed

        except ValidationError as e:
            logger.error(f"‚ùå Erreur parsing {response_model.__name__}: {e}")

            if fallback:
                logger.warning("Utilisation fallback")
                return response_model(**fallback)

            return None

    def call_with_parsing_sync(
        self,
        prompt: str,
        response_model: type[BaseModel],
        system_prompt: str = "",
        temperature: float | None = None,
        max_tokens: int = 1000,
        use_cache: bool = True,
        fallback: dict | None = None,
    ) -> BaseModel | None:
        """
        Version synchrone de call_with_parsing
        
        Wraps the async call_with_parsing in asyncio.run() for use from sync contexts
        like Streamlit.
        """
        # Essayer d'obtenir la boucle courante
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = None

        if loop is not None:
            # Si une boucle d'√©v√©nements est en cours, utiliser run_until_complete
            # Note: Cela peut √©chouer si on est d√©j√† dans une coroutine
            logger.warning("Event loop is running, trying to execute coroutine...")
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as pool:
                future = pool.submit(asyncio.run, 
                    self.call_with_parsing(
                        prompt=prompt,
                        response_model=response_model,
                        system_prompt=system_prompt,
                        temperature=temperature,
                        max_tokens=max_tokens,
                        use_cache=use_cache,
                        fallback=fallback,
                    )
                )
                return future.result()
        
        # Pas de boucle d'√©v√©nements, cr√©er une nouvelle et ex√©cuter
        return asyncio.run(
            self.call_with_parsing(
                prompt=prompt,
                response_model=response_model,
                system_prompt=system_prompt,
                temperature=temperature,
                max_tokens=max_tokens,
                use_cache=use_cache,
                fallback=fallback,
            )
        )

    @gerer_erreurs(afficher_dans_ui=True, valeur_fallback=[])
    async def call_with_list_parsing(
        self,
        prompt: str,
        item_model: type[BaseModel],
        list_key: str = "items",
        system_prompt: str = "",
        temperature: float | None = None,
        max_tokens: int = 2000,
        use_cache: bool = True,
        max_items: int | None = None,
    ) -> list[BaseModel]:
        """
        Appel IA avec parsing d'une liste

        Rate limiting + cache AUTO int√©gr√©s !
        """
        # Appel IA (rate limiting d√©j√† g√©r√©)
        response = await self.call_with_cache(
            prompt=prompt,
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            use_cache=use_cache,
        )

        if not response:
            return []

        # Parser liste
        try:
            from src.core.ai.parser import analyser_liste_reponse

            items = analyser_liste_reponse(
                reponse=response, modele_item=item_model, cle_liste=list_key, items_secours=[]
            )

            # Limiter nombre d'items
            if max_items and len(items) > max_items:
                items = items[:max_items]
                logger.info(f"Liste limit√©e √† {max_items} items")

            logger.info(f"‚úÖ {len(items)} items pars√©s ({item_model.__name__})")
            return items

        except Exception as e:
            logger.error(f"‚ùå Erreur parsing liste: {e}")
            return []

    def call_with_list_parsing_sync(
        self,
        prompt: str,
        item_model: type[BaseModel],
        list_key: str = "items",
        system_prompt: str = "",
        temperature: float | None = None,
        max_tokens: int = 2000,
        use_cache: bool = True,
        max_items: int | None = None,
    ) -> list[BaseModel]:
        """
        Version synchrone de call_with_list_parsing
        
        Wraps the async call_with_list_parsing in asyncio.run() for use from sync contexts
        like Streamlit.
        """
        # Essayer d'obtenir la boucle courante
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = None

        if loop is not None:
            # Si une boucle d'√©v√©nements est en cours, utiliser un thread
            logger.warning("Event loop is running, trying to execute coroutine in thread...")
            with concurrent.futures.ThreadPoolExecutor() as pool:
                future = pool.submit(asyncio.run, 
                    self.call_with_list_parsing(
                        prompt=prompt,
                        item_model=item_model,
                        list_key=list_key,
                        system_prompt=system_prompt,
                        temperature=temperature,
                        max_tokens=max_tokens,
                        use_cache=use_cache,
                        max_items=max_items,
                    )
                )
                return future.result()
        
        # Pas de boucle d'√©v√©nements, cr√©er une nouvelle et ex√©cuter
        return asyncio.run(
            self.call_with_list_parsing(
                prompt=prompt,
                item_model=item_model,
                list_key=list_key,
                system_prompt=system_prompt,
                temperature=temperature,
                max_tokens=max_tokens,
                use_cache=use_cache,
                max_items=max_items,
            )
        )

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # HELPERS PROMPTS STRUCTUR√âS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def build_json_prompt(
        self, context: str, task: str, json_schema: str, constraints: list[str] | None = None
    ) -> str:
        """Construit un prompt structur√© pour r√©ponse JSON"""
        prompt = f"{context}\n\n"
        prompt += f"T√ÇCHE: {task}\n\n"

        if constraints:
            prompt += "CONTRAINTES:\n"
            for constraint in constraints:
                prompt += f"- {constraint}\n"
            prompt += "\n"

        prompt += "FORMAT JSON ATTENDU:\n"
        prompt += f"{json_schema}\n\n"
        prompt += "IMPORTANT: R√©ponds UNIQUEMENT en JSON valide, sans texte avant ou apr√®s."

        return prompt

    def build_system_prompt(
        self, role: str, expertise: list[str], rules: list[str] | None = None
    ) -> str:
        """Construit un system prompt structur√©"""
        prompt = f"Tu es {role}.\n\n"

        prompt += "EXPERTISE:\n"
        for exp in expertise:
            prompt += f"- {exp}\n"
        prompt += "\n"

        if rules:
            prompt += "R√àGLES:\n"
            for rule in rules:
                prompt += f"- {rule}\n"
            prompt += "\n"

        prompt += "R√©ponds toujours en fran√ßais, de mani√®re claire et structur√©e."

        return prompt

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # M√âTRIQUES & DEBUG
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def get_cache_stats(self) -> dict:
        """Retourne statistiques cache"""
        return CacheIA.obtenir_statistiques()

    def get_rate_limit_stats(self) -> dict:
        """Retourne statistiques rate limiting"""
        return RateLimitIA.obtenir_statistiques()

    def clear_cache(self):
        """Vide le cache"""
        CacheIA.invalider_tout()
        logger.info(f"üóëÔ∏è Cache {self.cache_prefix} vid√©")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# MIXINS SP√âCIALIS√âS (INCHANG√âS mais maintenant UTILIS√âS !)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


class RecipeAIMixin:
    """Mixin pour fonctionnalit√©s IA recettes"""

    def build_recipe_context(
        self, filters: dict, ingredients_dispo: list[str] | None = None, nb_recettes: int = 3
    ) -> str:
        """Construit contexte pour g√©n√©ration recettes"""
        context = f"G√©n√®re {nb_recettes} recettes avec les crit√®res suivants:\n\n"

        if filters.get("saison"):
            context += f"- Saison: {filters['saison']}\n"
        if filters.get("type_repas"):
            context += f"- Type de repas: {filters['type_repas']}\n"
        if filters.get("difficulte"):
            context += f"- Difficult√© max: {filters['difficulte']}\n"
        if filters.get("is_quick"):
            context += "- Temps max: 30 minutes\n"

        if ingredients_dispo:
            context += "\nINGR√âDIENTS DISPONIBLES:\n"
            for ing in ingredients_dispo[:10]:
                context += f"- {ing}\n"
            context += "\nPrivil√©gier ces ingr√©dients si possible.\n"

        return context


class PlanningAIMixin:
    """Mixin pour fonctionnalit√©s IA planning"""

    def build_planning_context(self, config: dict, semaine_debut: str) -> str:
        """Construit contexte pour g√©n√©ration planning"""
        context = f"G√©n√®re un planning hebdomadaire pour la semaine du {semaine_debut}.\n\n"

        context += "CONFIGURATION FOYER:\n"
        context += f"- {config.get('nb_adultes', 2)} adultes\n"
        context += f"- {config.get('nb_enfants', 0)} enfants\n"

        if config.get("a_bebe"):
            context += "- Pr√©sence d'un jeune enfant (adapter certaines recettes pour texture/allerg√®nes)\n"

        if config.get("batch_cooking_actif"):
            context += "- Batch cooking activ√© (optimiser temps)\n"

        return context


class InventoryAIMixin:
    """Mixin pour fonctionnalit√©s IA inventaire"""

    def build_inventory_summary(self, inventaire: list[dict]) -> str:
        """Construit r√©sum√© inventaire pour IA"""
        from collections import defaultdict

        summary = f"INVENTAIRE ({len(inventaire)} articles):\n\n"

        # Grouper par cat√©gorie
        categories = defaultdict(list)
        for article in inventaire:
            cat = article.get("categorie", "Autre")
            categories[cat].append(article)

        # R√©sumer par cat√©gorie
        for cat, articles in categories.items():
            summary += f"{cat}:\n"
            for art in articles[:5]:  # Max 5 par cat√©gorie
                statut = art.get("statut", "ok")
                icon = "üî¥" if statut == "critique" else "‚ö†Ô∏è" if statut == "sous_seuil" else "‚úÖ"
                summary += f"  {icon} {art['nom']}: {art['quantite']} {art['unite']}\n"

            if len(articles) > 5:
                summary += f"  ... et {len(articles) - 5} autres\n"
            summary += "\n"

        # R√©sum√© statuts
        critiques = len([a for a in inventaire if a.get("statut") == "critique"])
        sous_seuil = len([a for a in inventaire if a.get("statut") == "sous_seuil"])

        summary += "STATUTS:\n"
        summary += f"- {critiques} articles critiques\n"
        summary += f"- {sous_seuil} articles sous le seuil\n"

        return summary


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# FACTORY
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


def create_base_ai_service(
    cache_prefix: str = "ai",
    default_ttl: int = 3600,
    default_temperature: float = 0.7,
    service_name: str = "unknown",
) -> BaseAIService:
    """Factory pour cr√©er un BaseAIService"""
    from src.core.ai import obtenir_client_ia

    client = obtenir_client_ia()

    return BaseAIService(
        client=client,
        cache_prefix=cache_prefix,
        default_ttl=default_ttl,
        default_temperature=default_temperature,
        service_name=service_name,
    )
