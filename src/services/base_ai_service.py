"""
Service IA de Base Centralis√©
√âlimine 60% de duplication entre services IA

Architecture:
- H√©rite de AIClient pour les appels API
- Int√®gre AIParser pour parsing automatique
- Cache et Rate Limiting int√©gr√©s
- Prompts templates r√©utilisables
"""
import logging
from typing import Dict, List, Optional, TypeVar, Type, Any
from pydantic import BaseModel
from datetime import datetime

from src.core.ai import AIClient, AIParser, parse_list_response
from src.core.cache import Cache, RateLimit
from src.core.errors import handle_errors, AIServiceError, RateLimitError

logger = logging.getLogger(__name__)

T = TypeVar('T', bound=BaseModel)


class BaseAIService:
    """
    Service IA de base avec fonctionnalit√©s communes

    Usage:
        class RecetteAIService(BaseAIService):
            def __init__(self, client: AIClient):
                super().__init__(client, cache_prefix="recettes_ai")
    """

    def __init__(
            self,
            client: AIClient,
            cache_prefix: str = "ai",
            default_ttl: int = 1800,
            default_temperature: float = 0.7
    ):
        """
        Args:
            client: Client IA
            cache_prefix: Pr√©fixe pour les cl√©s de cache
            default_ttl: TTL par d√©faut (30min)
            default_temperature: Temp√©rature par d√©faut
        """
        self.client = client
        self.cache_prefix = cache_prefix
        self.default_ttl = default_ttl
        self.default_temperature = default_temperature

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # APPELS IA G√âN√âRIQUES
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    @handle_errors(show_in_ui=True)
    async def call_with_parsing(
            self,
            prompt: str,
            response_model: Type[T],
            system_prompt: str = "",
            temperature: Optional[float] = None,
            max_tokens: int = 1000,
            use_cache: bool = True,
            cache_ttl: Optional[int] = None,
            fallback: Optional[Dict] = None,
            strict: bool = False
    ) -> T:
        """
        Appel IA avec parsing automatique

        Args:
            prompt: Prompt utilisateur
            response_model: Mod√®le Pydantic de retour
            system_prompt: Instructions syst√®me
            temperature: Temp√©rature (d√©faut: self.default_temperature)
            max_tokens: Tokens max
            use_cache: Utiliser cache
            cache_ttl: TTL cache (d√©faut: self.default_ttl)
            fallback: Donn√©es fallback si √©chec
            strict: Mode strict (raise si √©chec)

        Returns:
            Instance valid√©e du mod√®le
        """
        # V√©rifier rate limit
        can_call, error_msg = RateLimit.can_call()
        if not can_call:
            raise RateLimitError(error_msg, user_message=error_msg)

        # Temp√©rature
        temp = temperature if temperature is not None else self.default_temperature

        # Cl√© cache
        cache_key = None
        if use_cache:
            cache_key = self._generate_cache_key(prompt, system_prompt, temp)
            cached = Cache.get(cache_key, ttl=cache_ttl or self.default_ttl)
            if cached:
                logger.debug(f"Cache HIT: {cache_key[:50]}")
                return response_model(**cached)

        # Appel IA
        logger.info(f"ü§ñ Appel IA: {response_model.__name__}")

        response = await self.client.call(
            prompt=prompt,
            system_prompt=system_prompt or self._default_system_prompt(),
            temperature=temp,
            max_tokens=max_tokens,
            use_cache=False  # On g√®re le cache nous-m√™mes
        )

        # Parser
        result = AIParser.parse(
            response,
            response_model,
            fallback=fallback,
            strict=strict
        )

        # Cacher
        if use_cache and cache_key:
            Cache.set(cache_key, result.dict(), ttl=cache_ttl or self.default_ttl)

        logger.info(f"‚úÖ R√©sultat pars√©: {response_model.__name__}")
        return result

    @handle_errors(show_in_ui=True)
    async def call_with_list_parsing(
            self,
            prompt: str,
            item_model: Type[BaseModel],
            list_key: str = "items",
            system_prompt: str = "",
            temperature: Optional[float] = None,
            max_tokens: int = 2000,
            use_cache: bool = True,
            cache_ttl: Optional[int] = None,
            fallback_items: Optional[List[Dict]] = None,
            max_items: Optional[int] = None
    ) -> List[BaseModel]:
        """
        Appel IA retournant une liste

        Args:
            prompt: Prompt
            item_model: Mod√®le d'un item
            list_key: Cl√© JSON contenant la liste
            system_prompt: Instructions
            temperature: Temp√©rature
            max_tokens: Tokens max
            use_cache: Cache
            cache_ttl: TTL cache
            fallback_items: Items fallback
            max_items: Nombre max d'items √† retourner

        Returns:
            Liste d'items valid√©s
        """
        # V√©rifier rate limit
        can_call, error_msg = RateLimit.can_call()
        if not can_call:
            raise RateLimitError(error_msg, user_message=error_msg)

        # Temp√©rature
        temp = temperature if temperature is not None else self.default_temperature

        # Cl√© cache
        cache_key = None
        if use_cache:
            cache_key = self._generate_cache_key(prompt, system_prompt, temp)
            cached = Cache.get(cache_key, ttl=cache_ttl or self.default_ttl)
            if cached:
                logger.debug(f"Cache HIT: {cache_key[:50]}")
                return [item_model(**item) for item in cached]

        # Appel IA
        logger.info(f"ü§ñ Appel IA liste: {item_model.__name__}")

        response = await self.client.call(
            prompt=prompt,
            system_prompt=system_prompt or self._default_system_prompt(),
            temperature=temp,
            max_tokens=max_tokens,
            use_cache=False
        )

        # Parser liste
        items = parse_list_response(
            response,
            item_model,
            list_key=list_key,
            fallback_items=fallback_items or []
        )

        # Limiter si demand√©
        if max_items:
            items = items[:max_items]

        # Cacher
        if use_cache and cache_key:
            Cache.set(
                cache_key,
                [item.dict() for item in items],
                ttl=cache_ttl or self.default_ttl
            )

        logger.info(f"‚úÖ {len(items)} items pars√©s")
        return items

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # HELPERS PROMPTS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def build_json_prompt(
            self,
            context: str,
            task: str,
            json_schema: str,
            examples: Optional[str] = None,
            constraints: Optional[List[str]] = None
    ) -> str:
        """
        Construit un prompt structur√© pour JSON

        Args:
            context: Contexte m√©tier
            task: T√¢che √† accomplir
            json_schema: Sch√©ma JSON attendu
            examples: Exemples (optionnel)
            constraints: Contraintes (optionnel)

        Returns:
            Prompt structur√©
        """
        prompt = f"{context}\n\n"
        prompt += f"T√ÇCHE: {task}\n\n"

        if constraints:
            prompt += "CONTRAINTES:\n"
            for idx, constraint in enumerate(constraints, 1):
                prompt += f"{idx}. {constraint}\n"
            prompt += "\n"

        if examples:
            prompt += f"EXEMPLES:\n{examples}\n\n"

        prompt += f"FORMAT JSON:\n{json_schema}\n\n"
        prompt += "‚ö†Ô∏è UNIQUEMENT LE JSON, RIEN D'AUTRE !"

        return prompt

    def _default_system_prompt(self) -> str:
        """Prompt syst√®me par d√©faut"""
        return (
            "Tu es un assistant JSON expert. "
            "Tu g√©n√®res UNIQUEMENT du JSON valide. "
            "R√àGLES: "
            "1. Commence directement par { "
            "2. Termine directement par } "
            "3. Utilise UNIQUEMENT des doubles guillemets "
            "4. Pas de markdown (```json) "
            "5. Pas de texte avant/apr√®s le JSON"
        )

    def _generate_cache_key(
            self,
            prompt: str,
            system_prompt: str,
            temperature: float
    ) -> str:
        """G√©n√®re cl√© de cache unique"""
        import hashlib
        import json

        data = {
            "prefix": self.cache_prefix,
            "prompt": prompt[:500],  # Limiter pour performance
            "system": system_prompt[:200],
            "temp": temperature
        }

        cache_str = json.dumps(data, sort_keys=True)
        cache_hash = hashlib.md5(cache_str.encode()).hexdigest()

        return f"{self.cache_prefix}_{cache_hash}"

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # M√âTHODES √Ä OVERRIDE (TEMPLATES)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def get_fallback_data(self, request_type: str) -> Dict:
        """
        Retourne donn√©es de fallback

        √Ä override dans les classes filles
        """
        return {}

    def validate_response(self, data: Dict) -> bool:
        """
        Validation m√©tier custom

        √Ä override dans les classes filles
        """
        return True

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # M√âTRIQUES & DEBUG
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def get_usage_stats(self) -> Dict:
        """Stats d'utilisation du service"""
        usage = RateLimit.get_usage()
        cache_stats = Cache.get_stats()

        return {
            "service": self.__class__.__name__,
            "cache_prefix": self.cache_prefix,
            "rate_limit": {
                "calls_today": usage["calls_today"],
                "remaining": usage["remaining_today"]
            },
            "cache": {
                "hit_rate": cache_stats["hit_rate"],
                "total_keys": cache_stats["total_keys"]
            }
        }

    def clear_cache(self):
        """Vide le cache du service"""
        Cache.invalidate(self.cache_prefix)
        logger.info(f"Cache vid√©: {self.cache_prefix}")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# MIXINS SP√âCIALIS√âS
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class RecipeAIMixin:
    """Mixin pour services IA recettes"""

    def build_recipe_context(
            self,
            filters: Dict,
            ingredients: Optional[List[str]] = None,
            nb_recipes: int = 1
    ) -> str:
        """Construit contexte pour g√©n√©ration recettes"""
        context = f"G√©n√®re {nb_recipes} recette(s)"

        if filters.get("saison"):
            context += f" de saison {filters['saison']}"
        if filters.get("type_repas"):
            context += f" pour le {filters['type_repas']}"
        if filters.get("is_quick"):
            context += " rapides (<30min)"
        if ingredients:
            context += f" avec: {', '.join(ingredients[:5])}"

        return context


class PlanningAIMixin:
    """Mixin pour services IA planning"""

    def build_planning_context(
            self,
            config: Dict,
            semaine_debut: str
    ) -> str:
        """Construit contexte pour g√©n√©ration planning"""
        context = f"Planning semaine du {semaine_debut}\n"
        context += f"Foyer: {config.get('nb_adultes', 2)} adultes, "
        context += f"{config.get('nb_enfants', 0)} enfants\n"

        if config.get('a_bebe'):
            context += "üë∂ Mode b√©b√© activ√©\n"

        if config.get('batch_cooking_actif'):
            context += "üç≥ Batch cooking activ√©\n"

        return context


class InventoryAIMixin:
    """Mixin pour services IA inventaire"""

    def build_inventory_summary(self, inventaire: List[Dict]) -> str:
        """Construit r√©sum√© inventaire"""
        total = len(inventaire)

        # Compter par statut
        from collections import Counter
        statuts = Counter(i.get("statut", "ok") for i in inventaire)

        summary = f"Inventaire: {total} articles\n"
        summary += f"Stock bas: {statuts.get('sous_seuil', 0)}\n"
        summary += f"P√©remption proche: {statuts.get('peremption_proche', 0)}\n"
        summary += f"Critiques: {statuts.get('critique', 0)}"

        return summary