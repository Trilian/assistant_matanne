"""
Base AI Service - Service IA G√©n√©rique avec Mixins
Fournit fonctionnalit√©s communes pour tous les services IA
"""
import logging
from typing import Optional, Dict, List, Any, Type
from datetime import datetime
from pydantic import BaseModel, ValidationError

from src.core.ai import ClientIA, AnalyseurIA
from src.core.ai.cache import CacheIA
from src.core.errors import ErreurServiceIA, gerer_erreurs

logger = logging.getLogger(__name__)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# BASE AI SERVICE
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class BaseAIService:
    """
    Service IA de base avec fonctionnalit√©s communes

    Fonctionnalit√©s:
    - Appels IA avec cache s√©mantique automatique
    - Parsing JSON robuste
    - Retry automatique
    - Rate limiting
    - Gestion d'erreurs
    - Helpers pour prompts structur√©s
    """

    def __init__(
            self,
            client: ClientIA,
            cache_prefix: str = "ai",
            default_ttl: int = 3600,
            default_temperature: float = 0.7
    ):
        """
        Initialise le service IA

        Args:
            client: Client IA (ClientIA)
            cache_prefix: Pr√©fixe pour cl√©s cache
            default_ttl: TTL cache par d√©faut (secondes)
            default_temperature: Temp√©rature par d√©faut
        """
        self.client = client
        self.cache_prefix = cache_prefix
        self.default_ttl = default_ttl
        self.default_temperature = default_temperature

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # APPELS IA AVEC CACHE
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    @gerer_erreurs(afficher_dans_ui=True, valeur_fallback=None)
    async def call_with_cache(
            self,
            prompt: str,
            system_prompt: str = "",
            temperature: Optional[float] = None,
            max_tokens: int = 1000,
            use_cache: bool = True,
            category: Optional[str] = None
    ) -> Optional[str]:
        """
        Appel IA avec cache s√©mantique automatique

        Args:
            prompt: Prompt utilisateur
            system_prompt: Instructions syst√®me
            temperature: Temp√©rature (None = default)
            max_tokens: Tokens max
            use_cache: Utiliser le cache
            category: Cat√©gorie pour cache s√©mantique

        Returns:
            R√©ponse IA ou None si erreur
        """
        temp = temperature if temperature is not None else self.default_temperature
        cache_category = category or self.cache_prefix

        # V√©rifier cache s√©mantique
        if use_cache:
            cached = CacheIA.obtenir(
                prompt=prompt,
                systeme=system_prompt,
                temperature=temp,
            )

            if cached:
                logger.info(f"‚úÖ Cache HIT s√©mantique ({cache_category})")
                return cached

        # Appel IA
        response = await self.client.appeler(
            prompt=prompt,
            prompt_systeme=system_prompt,
            temperature=temp,
            max_tokens=max_tokens,
            utiliser_cache=False  # On g√®re le cache nous-m√™mes
        )

        # Sauvegarder dans cache
        if use_cache and response:
            CacheIA.definir(
                prompt=prompt,
                reponse=response,
                systeme=system_prompt,
                temperature=temp,
            )

        return response

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PARSING AVEC VALIDATION
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    @gerer_erreurs(afficher_dans_ui=True, valeur_fallback=None)
    async def call_with_parsing(
            self,
            prompt: str,
            response_model: Type[BaseModel],
            system_prompt: str = "",
            temperature: Optional[float] = None,
            max_tokens: int = 1000,
            use_cache: bool = True,
            fallback: Optional[Dict] = None
    ) -> Optional[BaseModel]:
        """
        Appel IA avec parsing automatique vers mod√®le Pydantic

        Args:
            prompt: Prompt
            response_model: Mod√®le Pydantic cible
            system_prompt: Instructions syst√®me
            temperature: Temp√©rature
            max_tokens: Tokens max
            use_cache: Utiliser cache
            fallback: Dict fallback si parsing √©choue

        Returns:
            Instance valid√©e du mod√®le ou None
        """
        # Appel IA
        response = await self.call_with_cache(
            prompt=prompt,
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            use_cache=use_cache
        )

        if not response:
            return None

        # Parser avec AnalyseurIA
        try:
            parsed = AnalyseurIA.analyser(
                reponse=response,
                modele=response_model,
                valeur_secours=fallback,
                strict=False
            )

            logger.info(f"‚úÖ Parsing r√©ussi: {response_model.__name__}")
            return parsed

        except ValidationError as e:
            logger.error(f"‚ùå Erreur parsing {response_model.__name__}: {e}")

            if fallback:
                logger.warning("Utilisation fallback")
                return response_model(**fallback)

            return None

    @gerer_erreurs(afficher_dans_ui=True, valeur_fallback=[])
    async def call_with_list_parsing(
            self,
            prompt: str,
            item_model: Type[BaseModel],
            list_key: str = "items",
            system_prompt: str = "",
            temperature: Optional[float] = None,
            max_tokens: int = 2000,
            use_cache: bool = True,
            max_items: Optional[int] = None
    ) -> List[BaseModel]:
        """
        Appel IA avec parsing d'une liste

        Args:
            prompt: Prompt
            item_model: Mod√®le d'un item
            list_key: Cl√© JSON contenant la liste
            system_prompt: Instructions syst√®me
            temperature: Temp√©rature
            max_tokens: Tokens max
            use_cache: Utiliser cache
            max_items: Nombre max d'items

        Returns:
            Liste d'items valid√©s
        """
        # Appel IA
        response = await self.call_with_cache(
            prompt=prompt,
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            use_cache=use_cache
        )

        if not response:
            return []

        # Parser liste
        try:
            from src.core.ai.parser import analyser_liste_reponse

            items = analyser_liste_reponse(
                reponse=response,
                modele_item=item_model,
                cle_liste=list_key,
                items_secours=[]
            )

            # Limiter nombre d'items
            if max_items and len(items) > max_items:
                items = items[:max_items]
                logger.info(f"Liste limit√©e √† {max_items} items")

            logger.info(f"‚úÖ {len(items)} items pars√©s")
            return items

        except Exception as e:
            logger.error(f"‚ùå Erreur parsing liste: {e}")
            return []

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # HELPERS PROMPTS STRUCTUR√âS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def build_json_prompt(
            self,
            context: str,
            task: str,
            json_schema: str,
            constraints: Optional[List[str]] = None
    ) -> str:
        """
        Construit un prompt structur√© pour r√©ponse JSON

        Args:
            context: Contexte m√©tier
            task: T√¢che √† accomplir
            json_schema: Sch√©ma JSON attendu
            constraints: Liste de contraintes

        Returns:
            Prompt format√©
        """
        prompt = f"{context}\n\n"
        prompt += f"T√ÇCHE: {task}\n\n"

        if constraints:
            prompt += "CONTRAINTES:\n"
            for constraint in constraints:
                prompt += f"- {constraint}\n"
            prompt += "\n"

        prompt += "FORMAT JSON ATTENDU:\n"
        prompt += f"{json_schema}\n\n"
        prompt += "IMPORTANT: R√©ponds UNIQUEMENT en JSON valide, sans texte avant ou apr√®s."

        return prompt

    def build_system_prompt(
            self,
            role: str,
            expertise: List[str],
            rules: Optional[List[str]] = None
    ) -> str:
        """
        Construit un system prompt structur√©

        Args:
            role: R√¥le de l'IA (ex: "Nutritionniste expert")
            expertise: Domaines d'expertise
            rules: R√®gles √† respecter

        Returns:
            System prompt format√©
        """
        prompt = f"Tu es {role}.\n\n"

        prompt += "EXPERTISE:\n"
        for exp in expertise:
            prompt += f"- {exp}\n"
        prompt += "\n"

        if rules:
            prompt += "R√àGLES:\n"
            for rule in rules:
                prompt += f"- {rule}\n"
            prompt += "\n"

        prompt += "R√©ponds toujours en fran√ßais, de mani√®re claire et structur√©e."

        return prompt

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # M√âTRIQUES & DEBUG
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def get_cache_stats(self) -> Dict:
        """Retourne statistiques cache"""
        return CacheIA.obtenir_statistiques()

    def clear_cache(self):
        """Vide le cache"""
        CacheIA.invalider_tout()
        logger.info(f"üóëÔ∏è Cache {self.cache_prefix} vid√©")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# MIXINS SP√âCIALIS√âS
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class RecipeAIMixin:
    """Mixin pour fonctionnalit√©s IA recettes"""

    def build_recipe_context(
            self,
            filters: Dict,
            ingredients_dispo: Optional[List[str]] = None,
            nb_recettes: int = 3
    ) -> str:
        """
        Construit contexte pour g√©n√©ration recettes

        Args:
            filters: Filtres (saison, type_repas, etc.)
            ingredients_dispo: Ingr√©dients disponibles
            nb_recettes: Nombre de recettes

        Returns:
            Contexte format√©
        """
        context = f"G√©n√®re {nb_recettes} recettes avec les crit√®res suivants:\n\n"

        if filters.get("saison"):
            context += f"- Saison: {filters['saison']}\n"
        if filters.get("type_repas"):
            context += f"- Type de repas: {filters['type_repas']}\n"
        if filters.get("difficulte"):
            context += f"- Difficult√© max: {filters['difficulte']}\n"
        if filters.get("is_quick"):
            context += f"- Temps max: 30 minutes\n"

        if ingredients_dispo:
            context += f"\nINGR√âDIENTS DISPONIBLES:\n"
            for ing in ingredients_dispo[:10]:
                context += f"- {ing}\n"
            context += "\nPrivil√©gier ces ingr√©dients si possible.\n"

        return context

    def build_recipe_adaptation_context(
            self,
            recette: Any,
            adaptation_type: str
    ) -> str:
        """
        Construit contexte pour adaptation recette

        Args:
            recette: Recette √† adapter
            adaptation_type: Type d'adaptation (b√©b√©, batch, etc.)

        Returns:
            Contexte format√©
        """
        context = f"RECETTE ORIGINALE: {recette.nom}\n\n"
        context += f"TYPE D'ADAPTATION: {adaptation_type}\n\n"

        context += "INGR√âDIENTS:\n"
        for ing in recette.ingredients:
            context += f"- {ing.quantite} {ing.unite} {ing.ingredient.nom}\n"

        context += "\n√âTAPES:\n"
        for etape in sorted(recette.etapes, key=lambda x: x.ordre):
            context += f"{etape.ordre}. {etape.description}\n"

        return context


class PlanningAIMixin:
    """Mixin pour fonctionnalit√©s IA planning"""

    def build_planning_context(
            self,
            config: Dict,
            semaine_debut: str
    ) -> str:
        """
        Construit contexte pour g√©n√©ration planning

        Args:
            config: Configuration foyer
            semaine_debut: Date d√©but semaine

        Returns:
            Contexte format√©
        """
        context = f"G√©n√®re un planning hebdomadaire pour la semaine du {semaine_debut}.\n\n"

        context += "CONFIGURATION FOYER:\n"
        context += f"- {config.get('nb_adultes', 2)} adultes\n"
        context += f"- {config.get('nb_enfants', 0)} enfants\n"

        if config.get('a_bebe'):
            context += "- Pr√©sence d'un b√©b√© (adapter certaines recettes)\n"

        if config.get('batch_cooking_actif'):
            context += "- Batch cooking activ√© (optimiser temps)\n"

        return context


class InventoryAIMixin:
    """Mixin pour fonctionnalit√©s IA inventaire"""

    def build_inventory_summary(
            self,
            inventaire: List[Dict]
    ) -> str:
        """
        Construit r√©sum√© inventaire pour IA

        Args:
            inventaire: Liste articles inventaire

        Returns:
            R√©sum√© format√©
        """
        summary = f"INVENTAIRE ({len(inventaire)} articles):\n\n"

        # Grouper par cat√©gorie
        from collections import defaultdict
        categories = defaultdict(list)

        for article in inventaire:
            cat = article.get("categorie", "Autre")
            categories[cat].append(article)

        # R√©sumer par cat√©gorie
        for cat, articles in categories.items():
            summary += f"{cat}:\n"
            for art in articles[:5]:  # Max 5 par cat√©gorie
                statut = art.get("statut", "ok")
                icon = "üî¥" if statut == "critique" else "‚ö†Ô∏è" if statut == "sous_seuil" else "‚úÖ"
                summary += f"  {icon} {art['nom']}: {art['quantite']} {art['unite']}\n"

            if len(articles) > 5:
                summary += f"  ... et {len(articles) - 5} autres\n"
            summary += "\n"

        # R√©sum√© statuts
        critiques = len([a for a in inventaire if a.get("statut") == "critique"])
        sous_seuil = len([a for a in inventaire if a.get("statut") == "sous_seuil"])

        summary += f"STATUTS:\n"
        summary += f"- {critiques} articles critiques\n"
        summary += f"- {sous_seuil} articles sous le seuil\n"

        return summary


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# FACTORY
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def create_base_ai_service(
        cache_prefix: str = "ai",
        default_ttl: int = 3600,
        default_temperature: float = 0.7
) -> BaseAIService:
    """
    Factory pour cr√©er un BaseAIService

    Args:
        cache_prefix: Pr√©fixe cache
        default_ttl: TTL par d√©faut
        default_temperature: Temp√©rature par d√©faut

    Returns:
        Instance BaseAIService
    """
    from src.core.ai import obtenir_client_ia

    client = obtenir_client_ia()

    return BaseAIService(
        client=client,
        cache_prefix=cache_prefix,
        default_ttl=default_ttl,
        default_temperature=default_temperature
    )