"""
Base AI Service - Service IA G√©n√©rique avec Rate Limiting Auto
Version am√©lior√©e avec gestion automatique des quotas et retry

[!] Ce module N'IMPORTE PAS streamlit ‚Äî d√©coupl√© de l'UI.
    Les erreurs sont logu√©es et propag√©es, l'affichage est g√©r√©
    par la couche UI via l'Event Bus.
"""

import logging
from datetime import datetime

from pydantic import BaseModel, ValidationError

from src.core.ai import AnalyseurIA, CircuitBreaker, ClientIA, RateLimitIA, obtenir_circuit
from src.core.ai.cache import CacheIA
from src.core.errors_base import ErreurLimiteDebit
from src.services.core.base.async_utils import sync_wrapper

logger = logging.getLogger(__name__)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# BASE AI SERVICE (AVEC RATE LIMITING AUTO)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


class BaseAIService:
    """
    Service IA de base avec fonctionnalit√©s communes

    Fonctionnalit√©s AUTO :
    - ‚úÖ Rate limiting avec retry intelligent
    - ‚úÖ Cache s√©mantique automatique
    - ‚úÖ Circuit breaker (protection service externe)
    - ‚úÖ Parsing JSON robuste
    - ‚úÖ Gestion d'erreurs unifi√©e
    - ‚úÖ Logging avec m√©triques
    """

    def __init__(
        self,
        client: ClientIA,
        cache_prefix: str = "ai",
        default_ttl: int = 3600,
        default_temperature: float = 0.7,
        service_name: str = "unknown",
        circuit_breaker: CircuitBreaker | None = None,
    ):
        """
        Initialise le service IA

        Args:
            client: Client IA (ClientIA)
            cache_prefix: Pr√©fixe pour cl√©s cache
            default_ttl: TTL cache par d√©faut (secondes)
            default_temperature: Temp√©rature par d√©faut
            service_name: Nom du service (pour analytics)
            circuit_breaker: Circuit breaker (auto-cr√©√© si None)
        """
        self.client = client
        self.cache_prefix = cache_prefix
        self.default_ttl = default_ttl
        self.default_temperature = default_temperature
        self.service_name = service_name
        self.circuit_breaker = circuit_breaker or obtenir_circuit(
            nom=f"ai_{service_name}",
            seuil_echecs=5,
            delai_reset=60.0,
        )

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # APPELS IA AVEC RATE LIMITING AUTO + CACHE
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    async def call_with_cache(
        self,
        prompt: str,
        system_prompt: str = "",
        temperature: float | None = None,
        max_tokens: int = 1000,
        use_cache: bool = True,
        category: str | None = None,
    ) -> str | None:
        """
        Appel IA avec rate limiting + cache automatiques

        Args:
            prompt: Prompt utilisateur
            system_prompt: Instructions syst√®me
            temperature: Temp√©rature (None = default)
            max_tokens: Tokens max
            use_cache: Utiliser le cache
            category: Cat√©gorie pour cache

        Returns:
            R√©ponse IA ou None si erreur

        Raises:
            ErreurLimiteDebit: Si quota atteint
        """
        # ‚úÖ V√©rifier que le client IA est disponible
        if self.client is None:
            logger.warning(f"‚ö†Ô∏è Client IA indispo ({self.service_name})")
            return None

        temp = temperature if temperature is not None else self.default_temperature
        cache_category = category or self.cache_prefix

        # ‚úÖ V√©rifier cache AVANT rate limit (√©conomise les quotas)
        if use_cache:
            cached = CacheIA.obtenir(
                prompt=prompt,
                systeme=system_prompt,
                temperature=temp,
            )

            if cached:
                logger.info(f"‚úÖ Cache HIT ({cache_category}) - Quota √©conomis√© !")
                return cached

        # ‚úÖ V√©rifier rate limit AUTO
        autorise, msg = RateLimitIA.peut_appeler()
        if not autorise:
            logger.warning(f"‚è≥ Rate limit: {msg}")
            raise ErreurLimiteDebit(msg, message_utilisateur=msg)

        # ‚úÖ V√©rifier circuit breaker AVANT l'appel
        from src.core.ai.circuit_breaker import EtatCircuit

        etat = self.circuit_breaker.etat
        if etat == EtatCircuit.OUVERT:
            logger.warning(f"‚ö° Circuit '{self.circuit_breaker.nom}' OUVERT ‚Äî appel bloqu√©")
            return None

        # Appel IA prot√©g√© par CircuitBreaker
        start_time = datetime.now()

        try:
            response = await self.client.appeler(
                prompt=prompt,
                prompt_systeme=system_prompt,
                temperature=temp,
                max_tokens=max_tokens,
                utiliser_cache=False,  # On g√®re le cache nous-m√™mes
            )
            self.circuit_breaker._enregistrer_succes()
        except Exception as e:
            self.circuit_breaker._enregistrer_echec()
            logger.warning("Appel IA √©chou√© (%s): %s", self.service_name, e)
            raise

        duration = (datetime.now() - start_time).total_seconds()

        # ‚úÖ Enregistrer appel AUTO (avec m√©triques)
        RateLimitIA.enregistrer_appel(
            service=self.service_name,
            tokens_utilises=len(response) if response else 0,  # Approximation
        )

        logger.info(
            f"‚úÖ Appel IA r√©ussi ({self.service_name}) - "
            f"{duration:.2f}s - {len(response) if response else 0} chars"
        )

        # Sauvegarder dans cache
        if use_cache and response:
            CacheIA.definir(
                prompt=prompt,
                reponse=response,
                systeme=system_prompt,
                temperature=temp,
            )

        return response

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PARSING AVEC VALIDATION
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    async def call_with_parsing(
        self,
        prompt: str,
        response_model: type[BaseModel],
        system_prompt: str = "",
        temperature: float | None = None,
        max_tokens: int = 1000,
        use_cache: bool = True,
        fallback: dict | None = None,
    ) -> BaseModel | None:
        """
        Appel IA avec parsing automatique vers mod√®le Pydantic

        Rate limiting + cache AUTO int√©gr√©s !
        """
        # Appel IA (rate limiting d√©j√† g√©r√©)
        response = await self.call_with_cache(
            prompt=prompt,
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            use_cache=use_cache,
        )

        if not response:
            return None

        # Parser avec AnalyseurIA
        try:
            parsed = AnalyseurIA.analyser(
                reponse=response, modele=response_model, valeur_secours=fallback, strict=False
            )

            logger.info(f"‚úÖ Parsing r√©ussi: {response_model.__name__}")
            return parsed

        except ValidationError as e:
            logger.error(f"‚ùå Erreur parsing {response_model.__name__}: {e}")

            if fallback:
                logger.warning("Utilisation fallback")
                return response_model(**fallback)

            return None

    # Version synchrone auto-g√©n√©r√©e via sync_wrapper
    call_with_parsing_sync = sync_wrapper(call_with_parsing)

    async def call_with_list_parsing(
        self,
        prompt: str,
        item_model: type[BaseModel],
        list_key: str = "items",
        system_prompt: str = "",
        temperature: float | None = None,
        max_tokens: int = 2000,
        use_cache: bool = True,
        max_items: int | None = None,
    ) -> list[BaseModel]:
        """
        Appel IA avec parsing d'une liste

        Rate limiting + cache AUTO int√©gr√©s !
        """
        # Appel IA (rate limiting d√©j√† g√©r√©)
        response = await self.call_with_cache(
            prompt=prompt,
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            use_cache=use_cache,
        )

        if not response:
            return []

        # Parser liste
        try:
            from src.core.ai.parser import analyser_liste_reponse

            items = analyser_liste_reponse(
                reponse=response, modele_item=item_model, cle_liste=list_key, items_secours=[]
            )

            # Limiter nombre d'items
            if max_items and len(items) > max_items:
                items = items[:max_items]
                logger.info(f"Liste limit√©e √† {max_items} items")

            logger.info(f"‚úÖ {len(items)} items pars√©s ({item_model.__name__})")
            return items

        except Exception as e:
            logger.error(f"‚ùå Erreur parsing liste: {e}")
            return []

    # Version synchrone auto-g√©n√©r√©e via sync_wrapper
    call_with_list_parsing_sync = sync_wrapper(call_with_list_parsing)

    async def call_with_json_parsing(
        self,
        prompt: str,
        response_model: type[BaseModel],
        system_prompt: str = "",
        temperature: float | None = None,
        max_tokens: int = 2000,
        use_cache: bool = True,
    ) -> BaseModel | None:
        """
        Appel IA avec parsing direct vers un mod√®le Pydantic unique.

        Args:
            prompt: Prompt utilisateur
            response_model: Mod√®le Pydantic attendu en r√©ponse
            system_prompt: Instructions syst√®me
            temperature: Temp√©rature (None = default)
            max_tokens: Tokens max
            use_cache: Utiliser le cache

        Returns:
            Instance du mod√®le Pydantic ou None si erreur
        """
        response = await self.call_with_cache(
            prompt=prompt,
            system_prompt=system_prompt
            or "Retourne uniquement du JSON valide, pas de markdown ni de texte.",
            temperature=temperature,
            max_tokens=max_tokens,
            use_cache=use_cache,
        )

        if not response:
            return None

        # D√©l√©guer le parsing JSON √† AnalyseurIA (nettoyage markdown inclus)
        try:
            parsed = AnalyseurIA.analyser(
                reponse=response,
                modele=response_model,
                valeur_secours=None,
                strict=False,
            )
            logger.info(f"‚úÖ JSON pars√© vers {response_model.__name__}")
            return parsed

        except ValidationError as e:
            logger.error(f"‚ùå Erreur validation Pydantic: {e}")
            return None
        except Exception as e:
            logger.error(f"‚ùå Erreur parsing inattendue: {e}")
            return None

    # Version synchrone auto-g√©n√©r√©e via sync_wrapper
    call_with_json_parsing_sync = sync_wrapper(call_with_json_parsing)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # HELPERS PROMPTS STRUCTUR√âS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def build_json_prompt(
        self, context: str, task: str, json_schema: str, constraints: list[str] | None = None
    ) -> str:
        """Construit un prompt structur√© pour r√©ponse JSON"""
        prompt = f"{context}\n\n"
        prompt += f"T√ÇCHE: {task}\n\n"

        if constraints:
            prompt += "CONTRAINTES:\n"
            for constraint in constraints:
                prompt += f"- {constraint}\n"
            prompt += "\n"

        prompt += "FORMAT JSON ATTENDU:\n"
        prompt += f"{json_schema}\n\n"
        prompt += "IMPORTANT: R√©ponds UNIQUEMENT en JSON valide, sans texte avant ou apr√®s."

        return prompt

    def build_system_prompt(
        self, role: str, expertise: list[str], rules: list[str] | None = None
    ) -> str:
        """Construit un system prompt structur√©"""
        prompt = f"Tu es {role}.\n\n"

        prompt += "EXPERTISE:\n"
        for exp in expertise:
            prompt += f"- {exp}\n"
        prompt += "\n"

        if rules:
            prompt += "R√àGLES:\n"
            for rule in rules:
                prompt += f"- {rule}\n"
            prompt += "\n"

        prompt += "R√©ponds toujours en fran√ßais, de mani√®re claire et structur√©e."

        return prompt

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # M√âTRIQUES & DEBUG
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def get_cache_stats(self) -> dict:
        """Retourne statistiques cache"""
        return CacheIA.obtenir_statistiques()

    def get_rate_limit_stats(self) -> dict:
        """Retourne statistiques rate limiting"""
        return RateLimitIA.obtenir_statistiques()

    def clear_cache(self):
        """Vide le cache"""
        CacheIA.invalider_tout()
        logger.info(f"üóëÔ∏è Cache {self.cache_prefix} vid√©")

    def get_circuit_breaker_stats(self) -> dict:
        """Retourne statistiques du circuit breaker."""
        return self.circuit_breaker.obtenir_statistiques()

    def reset_circuit_breaker(self):
        """Reset manuel du circuit breaker."""
        self.circuit_breaker.reset()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # API SAFE ‚Äî Retourne Result[T, ErrorInfo] au lieu de None
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    async def safe_call_with_cache(
        self,
        prompt: str,
        system_prompt: str = "",
        temperature: float | None = None,
        max_tokens: int = 1000,
        use_cache: bool = True,
        category: str | None = None,
    ):
        """Appel IA retournant Result au lieu de str|None.

        Returns:
            Success[str] si r√©ponse, Failure[ErrorInfo] si erreur/rate limit
        """
        from src.services.core.base.result import (
            ErrorCode,
            failure,
            from_exception,
            success,
        )

        try:
            response = await self.call_with_cache(
                prompt=prompt,
                system_prompt=system_prompt,
                temperature=temperature,
                max_tokens=max_tokens,
                use_cache=use_cache,
                category=category,
            )
            if response is None:
                return failure(
                    ErrorCode.AI_UNAVAILABLE,
                    "Client IA indisponible",
                    message_utilisateur="Le service IA est temporairement indisponible",
                    source=self.service_name,
                )
            return success(response)
        except ErreurLimiteDebit as e:
            return failure(
                ErrorCode.RATE_LIMITED,
                str(e),
                message_utilisateur=str(e),
                source=self.service_name,
            )
        except Exception as e:
            return from_exception(e, source=self.service_name)

    async def safe_call_with_parsing(
        self,
        prompt: str,
        response_model: type[BaseModel],
        system_prompt: str = "",
        temperature: float | None = None,
        max_tokens: int = 1000,
        use_cache: bool = True,
        fallback: dict | None = None,
    ):
        """Appel IA avec parsing Pydantic, retourne Result.

        Returns:
            Success[BaseModel] si pars√©, Failure[ErrorInfo] si √©chec
        """
        from src.services.core.base.result import (
            ErrorCode,
            failure,
            from_exception,
            success,
        )

        try:
            parsed = await self.call_with_parsing(
                prompt=prompt,
                response_model=response_model,
                system_prompt=system_prompt,
                temperature=temperature,
                max_tokens=max_tokens,
                use_cache=use_cache,
                fallback=fallback,
            )
            if parsed is None:
                return failure(
                    ErrorCode.PARSING_ERROR,
                    f"Impossible de parser la r√©ponse vers {response_model.__name__}",
                    source=self.service_name,
                )
            return success(parsed)
        except Exception as e:
            return from_exception(e, source=self.service_name)

    async def safe_call_with_list_parsing(
        self,
        prompt: str,
        item_model: type[BaseModel],
        list_key: str = "items",
        system_prompt: str = "",
        temperature: float | None = None,
        max_tokens: int = 2000,
        use_cache: bool = True,
        max_items: int | None = None,
    ):
        """Appel IA avec parsing liste, retourne Result.

        Returns:
            Success[list[BaseModel]], Failure[ErrorInfo] si erreur
        """
        from src.services.core.base.result import from_exception, success

        try:
            items = await self.call_with_list_parsing(
                prompt=prompt,
                item_model=item_model,
                list_key=list_key,
                system_prompt=system_prompt,
                temperature=temperature,
                max_tokens=max_tokens,
                use_cache=use_cache,
                max_items=max_items,
            )
            return success(items)
        except Exception as e:
            return from_exception(e, source=self.service_name)

    async def safe_call_with_json_parsing(
        self,
        prompt: str,
        response_model: type[BaseModel],
        system_prompt: str = "",
        temperature: float | None = None,
        max_tokens: int = 2000,
        use_cache: bool = True,
    ):
        """Appel IA avec parsing JSON, retourne Result.

        Returns:
            Success[BaseModel] si pars√©, Failure[ErrorInfo] si √©chec
        """
        from src.services.core.base.result import (
            ErrorCode,
            failure,
            from_exception,
            success,
        )

        try:
            parsed = await self.call_with_json_parsing(
                prompt=prompt,
                response_model=response_model,
                system_prompt=system_prompt,
                temperature=temperature,
                max_tokens=max_tokens,
                use_cache=use_cache,
            )
            if parsed is None:
                return failure(
                    ErrorCode.PARSING_ERROR,
                    f"Impossible de parser JSON vers {response_model.__name__}",
                    source=self.service_name,
                )
            return success(parsed)
        except Exception as e:
            return from_exception(e, source=self.service_name)

    # Versions synchrones des m√©thodes safe
    safe_call_with_cache_sync = sync_wrapper(safe_call_with_cache)
    safe_call_with_parsing_sync = sync_wrapper(safe_call_with_parsing)
    safe_call_with_list_parsing_sync = sync_wrapper(safe_call_with_list_parsing)
    safe_call_with_json_parsing_sync = sync_wrapper(safe_call_with_json_parsing)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # HEALTH CHECK ‚Äî Satisfait HealthCheckProtocol
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def health_check(self):
        """V√©rifie la sant√© du service IA (client, rate limit).

        Returns:
            ServiceHealth avec statut, latence et d√©tails quotas
        """
        import time

        from src.services.core.base.protocols import ServiceHealth, ServiceStatus

        start = time.perf_counter()
        details: dict = {"service_name": self.service_name}

        try:
            # V√©rifier client
            client_ok = self.client is not None
            details["client_available"] = client_ok

            # V√©rifier rate limit
            autorise, msg = RateLimitIA.peut_appeler()
            details["rate_limit_ok"] = autorise
            if not autorise:
                details["rate_limit_message"] = msg

            # V√©rifier circuit breaker
            cb_stats = self.circuit_breaker.obtenir_statistiques()
            details["circuit_breaker"] = cb_stats

            # R√©cup√©rer stats
            details["rate_limit_stats"] = self.get_rate_limit_stats()
            details["cache_stats"] = self.get_cache_stats()

            latency = (time.perf_counter() - start) * 1000

            if not client_ok:
                return ServiceHealth(
                    status=ServiceStatus.UNHEALTHY,
                    service_name=f"AI:{self.service_name}",
                    message="Client IA indisponible",
                    latency_ms=latency,
                    details=details,
                )

            if not autorise:
                return ServiceHealth(
                    status=ServiceStatus.DEGRADED,
                    service_name=f"AI:{self.service_name}",
                    message=f"Rate limit√©: {msg}",
                    latency_ms=latency,
                    details=details,
                )

            # V√©rifier √©tat du circuit breaker
            from src.core.ai.circuit_breaker import EtatCircuit

            if cb_stats["etat"] != EtatCircuit.FERME.value:
                return ServiceHealth(
                    status=ServiceStatus.DEGRADED,
                    service_name=f"AI:{self.service_name}",
                    message=f"Circuit breaker {cb_stats['etat']} "
                    f"({cb_stats['echecs_consecutifs']} √©checs)",
                    latency_ms=latency,
                    details=details,
                )

            return ServiceHealth(
                status=ServiceStatus.HEALTHY,
                service_name=f"AI:{self.service_name}",
                message="Service IA op√©rationnel",
                latency_ms=latency,
                details=details,
            )
        except Exception as e:
            latency = (time.perf_counter() - start) * 1000
            return ServiceHealth(
                status=ServiceStatus.UNHEALTHY,
                service_name=f"AI:{self.service_name}",
                message=f"Erreur health check: {e}",
                latency_ms=latency,
                details={"error": str(e)},
            )


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# MIXINS SP√âCIALIS√âS ‚Äî voir ai_mixins.py (source unique)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
from .ai_mixins import InventoryAIMixin, PlanningAIMixin, RecipeAIMixin  # noqa: E402, F401

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# FACTORY
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


def create_base_ai_service(
    cache_prefix: str = "ai",
    default_ttl: int = 3600,
    default_temperature: float = 0.7,
    service_name: str = "unknown",
    seuil_echecs: int = 5,
    delai_reset: float = 60.0,
) -> BaseAIService:
    """Factory pour cr√©er un BaseAIService avec CircuitBreaker.

    Args:
        cache_prefix: Pr√©fixe pour cl√©s cache
        default_ttl: TTL cache par d√©faut (secondes)
        default_temperature: Temp√©rature par d√©faut
        service_name: Nom du service (pour analytics)
        seuil_echecs: √âchecs cons√©cutifs avant ouverture du circuit
        delai_reset: D√©lai en secondes avant test de reprise
    """
    from src.core.ai import obtenir_client_ia

    client = obtenir_client_ia()
    cb = obtenir_circuit(
        nom=f"ai_{service_name}",
        seuil_echecs=seuil_echecs,
        delai_reset=delai_reset,
    )

    return BaseAIService(
        client=client,
        cache_prefix=cache_prefix,
        default_ttl=default_ttl,
        default_temperature=default_temperature,
        service_name=service_name,
        circuit_breaker=cb,
    )
