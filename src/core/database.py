"""
Connexion PostgreSQL et gestion des sessions
"""

from contextlib import contextmanager
from typing import Generator
from datetime import datetime
from sqlalchemy import create_engine, event, text
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.pool import NullPool, QueuePool
import logging

from src.core.config import settings
from src.core.models import Base

logger = logging.getLogger(__name__)


# ===================================
# CONFIGURATION ENGINE
# ===================================

def get_engine():
    """Cr√©e l'engine PostgreSQL avec configuration optimis√©e"""

    engine_kwargs = {
        "echo": settings.DEBUG,
        "future": True,
    }

    # Configuration du pool de connexions
    if settings.ENV == "production":
        engine_kwargs["poolclass"] = QueuePool
        engine_kwargs["pool_size"] = 10
        engine_kwargs["max_overflow"] = 20
        engine_kwargs["pool_pre_ping"] = True  # V√©rifier la connexion avant utilisation
        engine_kwargs["pool_recycle"] = 3600  # Recycler apr√®s 1h
    else:
        # En d√©veloppement, pool plus simple
        engine_kwargs["poolclass"] = NullPool if settings.ENV == "test" else QueuePool
        engine_kwargs["pool_size"] = 5
        engine_kwargs["max_overflow"] = 10

    engine = create_engine(settings.DATABASE_URL, **engine_kwargs)

    # Activer les contraintes de cl√©s √©trang√®res pour PostgreSQL
    @event.listens_for(engine, "connect")
    def set_postgresql_config(dbapi_conn, connection_record):
        cursor = dbapi_conn.cursor()
        cursor.execute("SET timezone='UTC'")
        cursor.close()

    return engine


# Engine global
engine = get_engine()

# Session factory
SessionLocal = sessionmaker(
    autocommit=False,
    autoflush=False,
    bind=engine,
    expire_on_commit=False
)


# ===================================
# GESTION DES SESSIONS
# ===================================

def get_session() -> Session:
    """
    Obtenir une session de base de donn√©es
    √Ä utiliser dans les fonctions normales
    """
    return SessionLocal()


@contextmanager
def get_db_context() -> Generator[Session, None, None]:
    """
    Context manager pour g√©rer automatiquement les sessions

    Usage:
        with get_db_context() as db:
            # Faire des op√©rations
            db.query(...)
    """
    db = SessionLocal()
    try:
        yield db
        db.commit()
    except Exception as e:
        db.rollback()
        logger.error(f"Erreur base de donn√©es: {e}")
        raise
    finally:
        db.close()


def get_db() -> Generator[Session, None, None]:
    """
    Dependency pour FastAPI/Streamlit
    √Ä utiliser avec Depends() si n√©cessaire
    """
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


# ===================================
# INITIALISATION & MIGRATIONS
# ===================================

def create_all_tables():
    """Cr√©e toutes les tables (d√©veloppement uniquement)"""
    logger.info("Cr√©ation des tables...")
    Base.metadata.create_all(bind=engine)
    logger.info("‚úÖ Tables cr√©√©es")


def drop_all_tables():
    """Supprime toutes les tables (DANGER!)"""
    logger.warning("‚ö†Ô∏è SUPPRESSION de toutes les tables")
    Base.metadata.drop_all(bind=engine)
    logger.warning("‚úÖ Tables supprim√©es")


def reset_database():
    """Reset complet de la base (d√©veloppement uniquement)"""
    if settings.ENV == "production":
        raise RuntimeError("‚ùå Reset impossible en production!")

    logger.warning("üîÑ Reset de la base de donn√©es...")
    drop_all_tables()
    create_all_tables()
    logger.info("‚úÖ Base r√©initialis√©e")


# ===================================
# V√âRIFICATIONS & SANT√â
# ===================================

def check_connection() -> bool:
    """V√©rifie que la connexion fonctionne"""
    try:
        with get_db_context() as db:
            db.execute(text("SELECT 1"))
        return True
    except Exception as e:
        logger.error(f"‚ùå Connexion DB √©chou√©e: {e}")
        return False


def get_db_info() -> dict:
    """Informations sur la base de donn√©es"""
    try:
        with get_db_context() as db:
            result = db.execute(text("""
                SELECT 
                    version() as version,
                    current_database() as database,
                    current_user as user
            """)).fetchone()

            return {
                "status": "connected",
                "version": result[0],
                "database": result[1],
                "user": result[2],
                "url": settings.DATABASE_URL.split("@")[1] if "@" in settings.DATABASE_URL else "hidden"
            }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }


# ===================================
# STREAMLIT CACHE
# ===================================

import streamlit as st
from functools import wraps


def cached_query(ttl: int = 300):
    """
    D√©corateur pour cacher les requ√™tes dans Streamlit

    Args:
        ttl: Time to live en secondes (d√©faut: 5 minutes)

    Usage:
        @cached_query(ttl=600)
        def get_recipes():
            with get_db_context() as db:
                return db.query(Recipe).all()
    """
    def decorator(func):
        @wraps(func)
        @st.cache_data(ttl=ttl)
        def wrapper(*args, **kwargs):
            return func(*args, **kwargs)
        return wrapper
    return decorator


# ===================================
# HELPERS SP√âCIFIQUES
# ===================================

def bulk_insert(model_class, data: list[dict]) -> int:
    """
    Insertion en masse efficace

    Args:
        model_class: Classe du mod√®le SQLAlchemy
        data: Liste de dictionnaires avec les donn√©es

    Returns:
        Nombre d'enregistrements ins√©r√©s
    """
    with get_db_context() as db:
        objects = [model_class(**item) for item in data]
        db.bulk_save_objects(objects)
        return len(objects)


def bulk_update(model_class, data: list[dict]) -> int:
    """
    Mise √† jour en masse

    Args:
        model_class: Classe du mod√®le
        data: Liste de dicts avec 'id' et les champs √† mettre √† jour

    Returns:
        Nombre d'enregistrements mis √† jour
    """
    with get_db_context() as db:
        updated = 0
        for item in data:
            item_id = item.pop('id')
            db.query(model_class).filter(
                model_class.id == item_id
            ).update(item)
            updated += 1
        return updated


def execute_raw_sql(sql: str, params: dict = None) -> list:
    """
    Ex√©cute du SQL brut (avec pr√©caution!)

    Args:
        sql: Requ√™te SQL
        params: Param√®tres nomm√©s

    Returns:
        Liste de r√©sultats
    """
    with get_db_context() as db:
        result = db.execute(text(sql), params or {})
        return result.fetchall()


# ===================================
# CLEANUP
# ===================================

def cleanup_old_logs(days: int = 90):
    """
    Nettoie les anciens logs IA et interactions

    Args:
        days: Garder les logs des X derniers jours
    """
    from datetime import timedelta
    from src.core.models import AIInteraction

    cutoff_date = datetime.utcnow() - timedelta(days=days)

    with get_db_context() as db:
        deleted = db.query(AIInteraction).filter(
            AIInteraction.created_at < cutoff_date
        ).delete()

        logger.info(f"üóëÔ∏è {deleted} logs IA supprim√©s (>{days} jours)")
        return deleted


def vacuum_database():
    """
    Optimise la base PostgreSQL (√† lancer r√©guli√®rement)
    """
    if settings.ENV == "production":
        logger.info("üßπ VACUUM de la base de donn√©es...")
        with engine.connect().execution_options(isolation_level="AUTOCOMMIT") as conn:
            conn.execute(text("VACUUM ANALYZE"))
        logger.info("‚úÖ VACUUM termin√©")