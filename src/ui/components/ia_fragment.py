"""
Widget IA isolÃ© â€” st.fragment + st.write_stream() combo.

Combine @st.fragment avec le streaming IA pour des rÃ©ponses
qui ne dÃ©clenchent pas de rerun global. Chaque widget IA
(chat, suggestions, Jules) peut streamer dans son propre fragment.

Innovation 1.1 du rapport d'audit â€” Impact immÃ©diat.

Usage:
    from src.ui.components.ia_fragment import (
        widget_ia_isole,
        chat_ia_fragment,
        suggestions_ia_fragment,
    )

    # Widget minimaliste
    widget_ia_isole(contexte="recettes")

    # Chat complet avec historique dans un fragment isolÃ©
    chat_ia_fragment("jules", context_extra={"age_mois": 20})

    # Suggestions rapides (genre "Qu'est-ce qu'on mange ?")
    suggestions_ia_fragment(
        prompt="SuggÃ¨re 3 recettes rapides pour ce soir",
        contexte="recettes",
    )
"""

from __future__ import annotations

import logging
from typing import Any

import streamlit as st

from src.core.state import rerun
from src.ui.fragments import _has_fragment, ui_fragment
from src.ui.keys import KeyNamespace
from src.ui.registry import composant_ui

logger = logging.getLogger(__name__)

_keys = KeyNamespace("ia_frag")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FRAGMENT IA ISOLÃ‰ â€” Chat minimaliste sans rerun global
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def _get_chat_service(contexte: str):
    """Obtient le service chat contextuel (import diffÃ©rÃ©)."""
    from src.ui.components.chat_contextuel import ChatContextuelService

    return ChatContextuelService(contexte)


@composant_ui(
    "ia",
    exemple='widget_ia_isole(contexte="recettes")',
    tags=("ia", "fragment", "streaming", "chat"),
)
def widget_ia_isole(
    contexte: str = "recettes",
    placeholder: str = "Demander Ã  l'IA...",
    system_prompt_override: str | None = None,
    hauteur_messages: int = 300,
    afficher_suggestions: bool = True,
) -> None:
    """Widget IA isolÃ© dans un st.fragment â€” pas de rerun global.

    Chaque interaction (envoi de message, rÃ©ponse streaming) reste
    confinÃ©e dans le fragment, Ã©vitant de recalculer toute la page.

    Args:
        contexte: Contexte IA ("recettes", "jules", "planning", "weekend", etc.)
        placeholder: Placeholder du chat_input
        system_prompt_override: Override du system prompt (optionnel)
        hauteur_messages: Hauteur du conteneur de messages (px)
        afficher_suggestions: Afficher les suggestions rapides
    """
    if _has_fragment():
        # Wrapper dans st.fragment pour isolation complÃ¨te
        @st.fragment
        def _widget_interne():
            _render_widget_ia(
                contexte=contexte,
                placeholder=placeholder,
                system_prompt_override=system_prompt_override,
                hauteur_messages=hauteur_messages,
                afficher_suggestions=afficher_suggestions,
            )

        _widget_interne()
    else:
        # Fallback sans fragment
        _render_widget_ia(
            contexte=contexte,
            placeholder=placeholder,
            system_prompt_override=system_prompt_override,
            hauteur_messages=hauteur_messages,
            afficher_suggestions=afficher_suggestions,
        )


def _render_widget_ia(
    contexte: str,
    placeholder: str,
    system_prompt_override: str | None,
    hauteur_messages: int,
    afficher_suggestions: bool,
) -> None:
    """Rendu interne du widget IA (appelÃ© dans ou hors fragment)."""
    sk = f"ia_frag_{contexte}_messages"
    if sk not in st.session_state:
        st.session_state[sk] = []

    messages: list[dict[str, str]] = st.session_state[sk]

    # Header compact
    col_t, col_a = st.columns([4, 1])
    with col_t:
        st.markdown(f"**ðŸ¤– IA {contexte.capitalize()}** `fragment isolÃ©`")
    with col_a:
        if st.button("ðŸ—‘ï¸", key=_keys("clear", contexte), help="Effacer"):
            st.session_state[sk] = []
            st.rerun()

    # Suggestions rapides (si vide)
    if afficher_suggestions and not messages:
        _afficher_suggestions_fragment(contexte)

    # Messages existants
    with st.container(height=hauteur_messages if messages else 100):
        for msg in messages:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

    # Input utilisateur â€” le streaming se fait DANS le fragment
    if prompt := st.chat_input(placeholder, key=_keys("input", contexte)):
        messages.append({"role": "user", "content": prompt})

        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            try:
                service = _get_chat_service(contexte)

                # st.write_stream dans le fragment = pas de rerun global
                response = st.write_stream(
                    service.call_with_streaming_sync(
                        prompt=_build_prompt(messages, contexte),
                        system_prompt=(system_prompt_override or service.config.get("system", "")),
                    )
                )

                resp_text = response if isinstance(response, str) else str(response)
                messages.append({"role": "assistant", "content": resp_text})

            except Exception as e:
                error_msg = f"âŒ Erreur: {e}"
                st.error(error_msg)
                messages.append({"role": "assistant", "content": error_msg})
                logger.error(f"Widget IA fragment ({contexte}): {e}")


def _build_prompt(messages: list[dict[str, str]], contexte: str) -> str:
    """Construit le prompt complet Ã  partir de l'historique."""
    historique = ""
    for msg in messages[:-1]:
        role = "Utilisateur" if msg["role"] == "user" else "Assistant"
        historique += f"{role}: {msg['content']}\n\n"

    derniere_question = messages[-1]["content"]

    return f"""Historique:
{historique}

Question: {derniere_question}

RÃ©ponds de maniÃ¨re concise et utile."""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CHAT IA FRAGMENT â€” Version complÃ¨te avec plus de features
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


@composant_ui(
    "ia",
    exemple='chat_ia_fragment("jules", context_extra={"age_mois": 20})',
    tags=("ia", "fragment", "chat", "complet"),
)
def chat_ia_fragment(
    contexte: str = "recettes",
    context_extra: dict[str, Any] | None = None,
    titre: str | None = None,
    hauteur: int = 400,
) -> None:
    """Chat IA complet dans un fragment isolÃ©.

    Version enrichie du widget avec :
    - Historique persistant en session
    - Suggestions contextuelles
    - Contexte supplÃ©mentaire (ingrÃ©dients, Ã¢ge, etc.)
    - Streaming sans rerun global

    Args:
        contexte: Type de contexte IA
        context_extra: DonnÃ©es contextuelles supplÃ©mentaires
        titre: Titre personnalisÃ© (optionnel)
        hauteur: Hauteur du conteneur
    """
    if _has_fragment():

        @st.fragment
        def _chat_interne():
            _render_chat_complet(contexte, context_extra, titre, hauteur)

        _chat_interne()
    else:
        _render_chat_complet(contexte, context_extra, titre, hauteur)


def _render_chat_complet(
    contexte: str,
    context_extra: dict[str, Any] | None,
    titre: str | None,
    hauteur: int,
) -> None:
    """Rendu du chat complet."""
    from src.ui.components.chat_contextuel import _PROMPTS_CONTEXTUELS

    config = _PROMPTS_CONTEXTUELS.get(contexte, _PROMPTS_CONTEXTUELS.get("recettes", {}))
    display_titre = titre or f"ðŸ’¬ Assistant {contexte.capitalize()}"

    sk = f"chat_frag_{contexte}_messages"
    if sk not in st.session_state:
        st.session_state[sk] = []

    messages: list[dict[str, str]] = st.session_state[sk]

    # Header
    col1, col2, col3 = st.columns([3, 1, 1])
    with col1:
        st.markdown(f"**{display_titre}** `fragment`")
    with col2:
        nb = len([m for m in messages if m["role"] == "user"])
        if nb:
            st.caption(f"ðŸ’¬ {nb} messages")
    with col3:
        if st.button("ðŸ—‘ï¸", key=_keys("chat_clear", contexte)):
            st.session_state[sk] = []
            st.rerun()

    # Suggestions
    if not messages and config.get("suggestions"):
        st.caption("ðŸ’¡ Suggestions:")
        cols = st.columns(2)
        for i, s in enumerate(config["suggestions"][:4]):
            with cols[i % 2]:
                if st.button(s, key=_keys("s", contexte, str(i)), use_container_width=True):
                    messages.append({"role": "user", "content": s})
                    st.rerun()

    # Messages
    with st.container(height=hauteur if messages else 120):
        for msg in messages:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

    # Input
    placeholder = config.get("placeholder", "Pose ta question...")

    if prompt := st.chat_input(placeholder, key=_keys("chat_input", contexte)):
        messages.append({"role": "user", "content": prompt})

        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            try:
                service = _get_chat_service(contexte)
                response = st.write_stream(service.streamer_reponse(messages, context_extra))
                resp = response if isinstance(response, str) else str(response)
                messages.append({"role": "assistant", "content": resp})
            except Exception as e:
                err = f"Erreur: {e}"
                st.error(err)
                messages.append({"role": "assistant", "content": err})
                logger.error(f"Chat fragment ({contexte}): {e}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SUGGESTIONS IA FRAGMENT â€” One-shot isolÃ©
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


@composant_ui(
    "ia",
    exemple='suggestions_ia_fragment("SuggÃ¨re 3 recettes rapides")',
    tags=("ia", "fragment", "suggestions"),
)
def suggestions_ia_fragment(
    prompt: str,
    contexte: str = "recettes",
    system_prompt: str = "",
    label: str = "ðŸ¤– Suggestions IA",
    temperature: float = 0.7,
    max_tokens: int = 1000,
) -> None:
    """GÃ©nÃ¨re des suggestions IA en streaming dans un fragment isolÃ©.

    IdÃ©al pour les boutons "Qu'est-ce qu'on mange ?" ou
    "SuggÃ¨re une activitÃ©" qui ne doivent pas rerun la page.

    Args:
        prompt: Le prompt Ã  envoyer
        contexte: Contexte IA
        system_prompt: System prompt override
        label: Label affichÃ© pendant le streaming
        temperature: TempÃ©rature IA
        max_tokens: Tokens max
    """
    if _has_fragment():

        @st.fragment
        def _suggestions():
            _render_suggestions(prompt, contexte, system_prompt, label, temperature, max_tokens)

        _suggestions()
    else:
        _render_suggestions(prompt, contexte, system_prompt, label, temperature, max_tokens)


def _render_suggestions(
    prompt: str,
    contexte: str,
    system_prompt: str,
    label: str,
    temperature: float,
    max_tokens: int,
) -> None:
    """Rendu des suggestions."""
    sk_result = f"suggest_frag_{hash(prompt)}"
    sk_loading = f"suggest_frag_loading_{hash(prompt)}"

    # Bouton pour dÃ©clencher
    if st.button(f"âœ¨ {label}", key=_keys("suggest_btn", str(hash(prompt)))):
        st.session_state[sk_loading] = True
        st.session_state.pop(sk_result, None)

    # Afficher le rÃ©sultat prÃ©cÃ©dent
    if sk_result in st.session_state:
        st.markdown(st.session_state[sk_result])

    # Streaming en cours
    if st.session_state.get(sk_loading):
        try:
            service = _get_chat_service(contexte)
            with st.chat_message("assistant"):
                response = st.write_stream(
                    service.call_with_streaming_sync(
                        prompt=prompt,
                        system_prompt=system_prompt,
                        temperature=temperature,
                        max_tokens=max_tokens,
                    )
                )
                resp_text = response if isinstance(response, str) else str(response)
                st.session_state[sk_result] = resp_text
                st.session_state[sk_loading] = False
        except Exception as e:
            st.error(f"Erreur: {e}")
            st.session_state[sk_loading] = False
            logger.error(f"Suggestions fragment: {e}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SUGGESTIONS RAPIDES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_SUGGESTIONS: dict[str, list[str]] = {
    "recettes": ["Recette rapide ce soir", "IdÃ©e batch cooking", "Dessert facile"],
    "jules": ["ActivitÃ© calme", "Jeu moteur", "Repas adaptÃ© 20 mois"],
    "planning": ["Planifier la semaine", "Optimiser les courses"],
    "weekend": ["Sortie ce weekend", "ActivitÃ© pluie"],
    "famille": ["Organiser le weekend", "ActivitÃ© en famille"],
    "maison": ["Entretien du mois", "RÃ©duire les charges"],
}


def _afficher_suggestions_fragment(contexte: str) -> None:
    """Affiche les suggestions rapides pour dÃ©marrer."""
    suggestions = _SUGGESTIONS.get(contexte, _SUGGESTIONS.get("recettes", []))
    if not suggestions:
        return

    sk = f"ia_frag_{contexte}_messages"
    st.caption("ðŸ’¡ Suggestions:")
    cols = st.columns(min(3, len(suggestions)))
    for i, s in enumerate(suggestions):
        with cols[i % len(cols)]:
            if st.button(s, key=_keys("quick", contexte, str(i)), use_container_width=True):
                if sk in st.session_state:
                    st.session_state[sk].append({"role": "user", "content": s})
                    st.rerun()


__all__ = [
    "widget_ia_isole",
    "chat_ia_fragment",
    "suggestions_ia_fragment",
]
